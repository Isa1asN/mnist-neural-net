{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_data = pd.read_csv('./../data/mnist_train.csv')\n",
    "ts_data = pd.read_csv('./../data/mnist_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>1x1</th>\n",
       "      <th>1x2</th>\n",
       "      <th>1x3</th>\n",
       "      <th>1x4</th>\n",
       "      <th>1x5</th>\n",
       "      <th>1x6</th>\n",
       "      <th>1x7</th>\n",
       "      <th>1x8</th>\n",
       "      <th>1x9</th>\n",
       "      <th>...</th>\n",
       "      <th>28x19</th>\n",
       "      <th>28x20</th>\n",
       "      <th>28x21</th>\n",
       "      <th>28x22</th>\n",
       "      <th>28x23</th>\n",
       "      <th>28x24</th>\n",
       "      <th>28x25</th>\n",
       "      <th>28x26</th>\n",
       "      <th>28x27</th>\n",
       "      <th>28x28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.0</td>\n",
       "      <td>60000.0</td>\n",
       "      <td>60000.0</td>\n",
       "      <td>60000.0</td>\n",
       "      <td>60000.0</td>\n",
       "      <td>60000.0</td>\n",
       "      <td>60000.0</td>\n",
       "      <td>60000.0</td>\n",
       "      <td>60000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.000000</td>\n",
       "      <td>60000.0000</td>\n",
       "      <td>60000.0</td>\n",
       "      <td>60000.0</td>\n",
       "      <td>60000.0</td>\n",
       "      <td>60000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.453933</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200433</td>\n",
       "      <td>0.088867</td>\n",
       "      <td>0.045633</td>\n",
       "      <td>0.019283</td>\n",
       "      <td>0.015117</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.889270</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.042472</td>\n",
       "      <td>3.956189</td>\n",
       "      <td>2.839845</td>\n",
       "      <td>1.686770</td>\n",
       "      <td>1.678283</td>\n",
       "      <td>0.3466</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>253.000000</td>\n",
       "      <td>253.000000</td>\n",
       "      <td>254.000000</td>\n",
       "      <td>62.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              label      1x1      1x2      1x3      1x4      1x5      1x6   \n",
       "count  60000.000000  60000.0  60000.0  60000.0  60000.0  60000.0  60000.0  \\\n",
       "mean       4.453933      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "std        2.889270      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "min        0.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "25%        2.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "50%        4.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "75%        7.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "max        9.000000      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "           1x7      1x8      1x9  ...         28x19         28x20   \n",
       "count  60000.0  60000.0  60000.0  ...  60000.000000  60000.000000  \\\n",
       "mean       0.0      0.0      0.0  ...      0.200433      0.088867   \n",
       "std        0.0      0.0      0.0  ...      6.042472      3.956189   \n",
       "min        0.0      0.0      0.0  ...      0.000000      0.000000   \n",
       "25%        0.0      0.0      0.0  ...      0.000000      0.000000   \n",
       "50%        0.0      0.0      0.0  ...      0.000000      0.000000   \n",
       "75%        0.0      0.0      0.0  ...      0.000000      0.000000   \n",
       "max        0.0      0.0      0.0  ...    254.000000    254.000000   \n",
       "\n",
       "              28x21         28x22         28x23       28x24    28x25    28x26   \n",
       "count  60000.000000  60000.000000  60000.000000  60000.0000  60000.0  60000.0  \\\n",
       "mean       0.045633      0.019283      0.015117      0.0020      0.0      0.0   \n",
       "std        2.839845      1.686770      1.678283      0.3466      0.0      0.0   \n",
       "min        0.000000      0.000000      0.000000      0.0000      0.0      0.0   \n",
       "25%        0.000000      0.000000      0.000000      0.0000      0.0      0.0   \n",
       "50%        0.000000      0.000000      0.000000      0.0000      0.0      0.0   \n",
       "75%        0.000000      0.000000      0.000000      0.0000      0.0      0.0   \n",
       "max      253.000000    253.000000    254.000000     62.0000      0.0      0.0   \n",
       "\n",
       "         28x27    28x28  \n",
       "count  60000.0  60000.0  \n",
       "mean       0.0      0.0  \n",
       "std        0.0      0.0  \n",
       "min        0.0      0.0  \n",
       "25%        0.0      0.0  \n",
       "50%        0.0      0.0  \n",
       "75%        0.0      0.0  \n",
       "max        0.0      0.0  \n",
       "\n",
       "[8 rows x 785 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_data.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tr_data.drop(columns='label').values\n",
    "train_labels = tr_data.label.values\n",
    "test_data = ts_data.drop(columns='label').values\n",
    "test_labels = ts_data.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data/255\n",
    "test_data = test_data/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation:\n",
    "    def softmax(self, z):\n",
    "        return 1/sum(np.exp(z)) * np.exp(z)\n",
    "\n",
    "    def stable_softmax(self, x):\n",
    "        # \"\"\" ... numerically stable way.\"\"\"\n",
    "        shiftx = x - np.max(x)\n",
    "        exps = np.exp(shiftx)\n",
    "        return exps / np.sum(exps)\n",
    "\n",
    "    def relu(self, z):\n",
    "        return np.maximum(0, z)\n",
    "\n",
    "    def tanh(self, z):\n",
    "        return np.tanh(z)\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return 1/(1 + np.exp(-z))\n",
    "    \n",
    "    def use(self, string):\n",
    "       match string:\n",
    "        case 'relu':\n",
    "            return self.relu\n",
    "        case 'sigmoid':\n",
    "            return self.sigmoid\n",
    "        case 'softmax':\n",
    "            return self.softmax\n",
    "        case 'tanh':\n",
    "            return self.tanh\n",
    "        case 'stable_softmax':\n",
    "            return self.stable_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFunctions:\n",
    "    def mean_squared_error(self, y_true, y_pred):\n",
    "        mse = np.mean((y_true - y_pred)**2)\n",
    "        return mse\n",
    "    \n",
    "    def categorical_cross_entropy(self, y_hat, label):\n",
    "        return -sum(np.multiply(np.log(y_hat), one_hot(label)))\n",
    "    \n",
    "    def grad_relu(self, x):\n",
    "        return (x > 0).astype(int)\n",
    "    \n",
    "    def decay_alpha(i):\n",
    "        return( 0.01 if i <=12 else (0.001 if i <= 24 else (0.0001)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder:\n",
    "    def one_hot(self, label):\n",
    "        arr = np.zeros(shape = (10,1))\n",
    "        arr[label] = 1\n",
    "        return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics:\n",
    "    def accuracy(self, y_hat, true):\n",
    "        return 100*np.mean(np.where(y_hat == np.reshape(true,len(true)), 1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, train_data, train_labels, test_data, test_labels, hidden_layer_u, learning_rate=0.01):\n",
    "\n",
    "        self.x_train = train_data\n",
    "        self.x_test = test_data\n",
    "        self.y_train = train_labels\n",
    "        self.y_test = test_labels\n",
    "\n",
    "        self.layer1 = 784\n",
    "        self.layer2 = hidden_layer_u\n",
    "        self.layer3 = 10\n",
    "\n",
    "        self.W1 = np.random.randn(self.layer2,self.layer1)/np.sqrt(self.layer1)\n",
    "        self.b1 = np.zeros(shape = (self.layer2,1))\n",
    "        self.W2 = np.random.randn(self.layer3,self.layer2)/np.sqrt(self.layer2)\n",
    "        self.b2 = np.zeros(shape = (self.layer3,1))\n",
    "\n",
    "    def summary(self):\n",
    "        print('Model configurations: ')\n",
    "        print('W1 has dim', self.W1.shape)\n",
    "        print('b1 has dim', self.b1.shape)\n",
    "        print('W2 has dim', self.W2.shape)\n",
    "        print('b2 has dim', self.b2.shape)\n",
    "        print('Input has dim', self.layer1)\n",
    "        print('Hidden layer has dim', self.layer2)\n",
    "        print('Output has dim', self.layer3)\n",
    "\n",
    "    def forward(self,x):\n",
    "        \n",
    "        self.z1 = np.array([self.W1.dot(x)]).transpose() + self.b1\n",
    "        vec_rectified_linear_unit = np.vectorize(Activation().use('relu'))\n",
    "        self.h = vec_rectified_linear_unit(self.z1)\n",
    "        self.h = self.h.transpose()[0]\n",
    "        self.z2 = np.array([self.W2.dot(self.h)]).transpose() + self.b2\n",
    "        self.y_hat = Activation().use('stable_softmax')(self.z2)    \n",
    "        \n",
    "    def back_prop(self,Ytr1,Xtr1):\n",
    "        \n",
    "        y_hat = self.y_hat\n",
    "        h = self.h\n",
    "        z1 = self.z1\n",
    "        \n",
    "        true = Encoder().one_hot(Ytr1)\n",
    "\n",
    "        diff_outer = -(true - y_hat)\n",
    "\n",
    "        del_b2 = diff_outer\n",
    "        del_W2 = np.matmul(diff_outer,np.reshape(h,(1,self.l2)))\n",
    "        DEL = self.W2.transpose().dot(diff_outer)\n",
    "\n",
    "        NAB = np.multiply(DEL, grad_rectified_linear_unit(z1))\n",
    "        del_b1 = NAB\n",
    "        del_W1 = np.matmul(np.reshape(NAB, (self.l2,1)), np.reshape(Xtr1, (1,self.l1)))\n",
    "\n",
    "        self.W2 = self.W2 - alpha * del_W2\n",
    "        self.b2 = self.b2 - alpha * del_b2\n",
    "        self.b1 = self.b1 - alpha * del_b1\n",
    "        self.W1 = self.W1 - alpha * del_W1\n",
    "                \n",
    "    def predict(self,x):\n",
    "        y_hat_lab = np.zeros(shape = (len(x)))\n",
    "        for i in range(len(x)):\n",
    "            self.forward(x[i])\n",
    "            y_hat_lab[i] = np.argmax(self.y_hat)\n",
    "        return y_hat_lab\n",
    "\n",
    "    def train(self, epochs):\n",
    "        x_learn, x_val, y_learning, y_val = train_test_split(self.x_train, self.y_train)\n",
    "        L = [i for i in range(0,len(x_learn))]\n",
    "        print('------------------------')\n",
    "        print(\"training the network\")\n",
    "\n",
    "        for j in range(epochs):\n",
    "            alpha = decay_alpha(j)\n",
    "            loss = 0\n",
    "            np.random.shuffle(L)\n",
    "            \n",
    "            for i in L:\n",
    "                self.forward(x_learn[i])\n",
    "                self.back_prop(y_learning[i], x_learn[i])\n",
    "                \n",
    "                loss = loss + LossFunctions().categorical_cross_entropy(self.y_hat, y_learning[i])\n",
    "            \n",
    "            loss = loss/len(L)\n",
    "            predicted_labels_validation = self.predict(x_val)\n",
    "            print('Epoch :', j)\n",
    "            print('Loss ->', loss[0])\n",
    "            print('accuracy ->', Metrics().accuracy(predicted_labels_validation, y_val))\n",
    "            print('------------------------')\n",
    "        print('---------training complete----------')\n",
    "\n",
    "    def run_test(self):\n",
    "        temp = self.x_test@self.W1.T + self.b1.T\n",
    "        temp = np.clip(temp,a_min=0, a_max=temp.max())\n",
    "        temp = temp@self.W2.T + self.b2.T\n",
    "        temp = np.exp(temp)\n",
    "        temp2 = temp.sum(axis=1)\n",
    "        temp = temp/temp2.reshape(-1,1)\n",
    "        preds = np.argmax(temp,axis=1)\n",
    "        print('the testing accuracy is :', Metrics().accuracy(preds, self.y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(train_data, train_labels, test_data, test_labels, 100, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model configurations: \n",
      "W1 has dim (100, 784)\n",
      "b1 has dim (100, 1)\n",
      "W2 has dim (10, 100)\n",
      "b2 has dim (10, 1)\n",
      "Input has dim 784\n",
      "Hidden layer has dim 100\n",
      "Output has dim 10\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "training the network\n",
      "Epoch : 0\n",
      "Loss -> 0.06488945574244043\n",
      "accuracy -> 12.693333333333335\n",
      "------------------------\n",
      "---------training complete----------\n"
     ]
    }
   ],
   "source": [
    "model.train(epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the testing accuracy is : 12.790000000000001\n"
     ]
    }
   ],
   "source": [
    "model.run_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def rectified_linear_unit(x):\n",
    "#     if x<0:\n",
    "#         return(0)\n",
    "#     else :\n",
    "#         return(x)\n",
    "\n",
    "# def grad_rectified_linear_unit(x):\n",
    "#     x[x>0] = 1\n",
    "#     x[x<=0] = 0\n",
    "#     return x\n",
    "    \n",
    "# def one_hot(label):\n",
    "#     arr = np.zeros(shape = (10,1))\n",
    "#     arr[label] = 1\n",
    "#     return arr\n",
    "\n",
    "# def categorical_cross_entropy(y_hat, label):\n",
    "#     return -sum(np.multiply(np.log(y_hat), one_hot(label)))\n",
    "\n",
    "# def stable_softmax(X):\n",
    "#     temp2 = np.exp(X - np.max(X))\n",
    "#     return temp2 / np.sum(temp2)\n",
    "\n",
    "# def decay_alpha(i):\n",
    "    # return( 0.01 if i <=12 else (0.001 if i <= 24 else (0.0001)) )\n",
    "    \n",
    "# def accuracy(y_hat, true):\n",
    "#     return 100*np.mean(np.where(y_hat == np.reshape(true,len(true)), 1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model configurations are as follows :\n",
      "-------------------------------------\n",
      "layer 1\n",
      "W1 has dim (100, 784)\n",
      "b1 has dim (100, 1)\n",
      "W2 has dim (10, 100)\n",
      "b2 has dim (10, 1)\n",
      "Input has dim 784\n",
      "Hidden layer has dim 100\n",
      "Output has dim 10\n",
      "\n",
      "------------------------\n",
      "start training the net\n",
      "Epoch Summary for epoch: 0\n",
      "Loss -> 0.2780669935288418\n",
      "accuracy -> 94.72\n",
      "\n",
      "the testing accuracy of the classifier is : [0.9556]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class neural_network:\n",
    "        \n",
    "    def __init__(self,lr=0.001,loss_func='categorical_cross_entropy',MNIST=True,xtr=None,xts=None,ytr=None,yts=None):\n",
    "        \n",
    "        self.lr = lr\n",
    "        self.loss_func = 'categorical_cross_entropy'\n",
    "        \n",
    "        if MNIST != True :\n",
    "            assert(None not in [xtr,xts,ytr,yts])\n",
    "            self.x_train = xtr\n",
    "            self.x_test = xts\n",
    "            self.y_train = ytr\n",
    "            self.y_test = yts\n",
    "\n",
    "        else :\n",
    "            self.prepare_data()\n",
    "            \n",
    "    def prepare_data(self):\n",
    "        \n",
    "        self.x_train = train_data\n",
    "        self.x_test = test_data\n",
    "        self.y_train = train_labels\n",
    "        self.y_test = test_labels\n",
    "        \n",
    "            \n",
    "        \n",
    "            \n",
    "    def print_model(self):\n",
    "        print('')\n",
    "        print('Model configurations are as follows :')\n",
    "        print('-------------------------------------')\n",
    "        \n",
    "        print('layer 1')\n",
    "        print('W1 has dim', self.W1.shape)\n",
    "        print('b1 has dim', self.b1.shape)\n",
    "        print('W2 has dim', self.W2.shape)\n",
    "        print('b2 has dim', self.b2.shape)\n",
    "        print('Input has dim', self.l1)\n",
    "        print('Hidden layer has dim', self.l2)\n",
    "        print('Output has dim', self.l3)\n",
    "            \n",
    "    def create_model(self, L2):\n",
    "        \n",
    "        self.l1 = 784\n",
    "        self.l2 = L2\n",
    "        self.l3 = 10\n",
    "\n",
    "        self.W1 = np.random.randn(self.l2,self.l1)/np.sqrt(self.l1)\n",
    "        self.b1 = np.zeros(shape = (self.l2,1))\n",
    "        self.W2 = np.random.randn(self.l3,self.l2)/np.sqrt(self.l2)\n",
    "        self.b2 = np.zeros(shape = (self.l3,1))\n",
    "    \n",
    "    def forward(self,x):\n",
    "        \n",
    "        self.z1 = np.array([self.W1.dot(x)]).transpose() + self.b1\n",
    "        vec_rectified_linear_unit = np.vectorize(rectified_linear_unit)\n",
    "        self.h = vec_rectified_linear_unit(self.z1)\n",
    "        self.h = self.h.transpose()[0]\n",
    "        self.z2 = np.array([self.W2.dot(self.h)]).transpose() + self.b2\n",
    "        self.y_hat = stable_softmax(self.z2)\n",
    "    \n",
    "    def predict(self,x):\n",
    "        y_hat_lab = np.zeros(shape = (len(x)))\n",
    "        for i in range(len(x)):\n",
    "            self.forward(x[i])\n",
    "            y_hat_lab[i] = np.argmax(self.y_hat)\n",
    "        return y_hat_lab\n",
    "    \n",
    "    def back_prop(self,Ytr1,Xtr1):\n",
    "        \n",
    "        y_hat = self.y_hat\n",
    "        h = self.h\n",
    "        z1 = self.z1\n",
    "        \n",
    "        true = one_hot(Ytr1)\n",
    "\n",
    "        diff_outer = -(true - y_hat)\n",
    "\n",
    "        del_b2 = diff_outer\n",
    "        del_W2 = np.matmul(diff_outer,np.reshape(h,(1,self.l2)))\n",
    "        DEL = self.W2.transpose().dot(diff_outer)\n",
    "\n",
    "        NAB = np.multiply(DEL, grad_rectified_linear_unit(z1))\n",
    "        del_b1 = NAB\n",
    "        del_W1 = np.matmul(np.reshape(NAB, (self.l2,1)), np.reshape(Xtr1, (1,self.l1)))\n",
    "\n",
    "        self.W2 = self.W2 - alpha * del_W2\n",
    "        self.b2 = self.b2 - alpha * del_b2\n",
    "        self.b1 = self.b1 - alpha * del_b1\n",
    "        self.W1 = self.W1 - alpha * del_W1\n",
    "        \n",
    "        \n",
    "    def test_and_summarize(self):\n",
    "        temp = self.x_test@self.W1.T+self.b1.T\n",
    "        temp = np.clip(temp,a_min=0,a_max=temp.max())\n",
    "        temp = temp@self.W2.T+self.b2.T\n",
    "        temp = np.exp(temp)\n",
    "        temp2 = temp.sum(axis=1)\n",
    "        temp = temp/temp2.reshape(-1,1)\n",
    "        preds = np.argmax(temp,axis=1)\n",
    "        print('the testing accuracy of the classifier is :',sum(preds.reshape(-1,1) == self.y_test.reshape(-1,1))/len(self.y_test))\n",
    "        \n",
    "        \n",
    "        \n",
    "NN = neural_network()\n",
    "NN.create_model(100)\n",
    "NN.print_model()\n",
    "\n",
    "\n",
    "\n",
    "x_learn, x_val, y_learning, y_val = train_test_split(NN.x_train, NN.y_train)\n",
    "L = [i for i in range(0,len(x_learn))]\n",
    "epochs = 1\n",
    "print('')\n",
    "print('------------------------')\n",
    "print(\"start training the net\")\n",
    "\n",
    "for j in range(epochs):\n",
    "    \n",
    "    #IMPLEMETING SGD ALGORITHM\n",
    "    \n",
    "    alpha = decay_alpha(j)\n",
    "    loss = 0\n",
    "    np.random.shuffle(L)\n",
    "    \n",
    "    for i in L:\n",
    "        NN.forward(x_learn[i])\n",
    "        NN.back_prop(y_learning[i],x_learn[i])\n",
    "        \n",
    "        loss = loss + categorical_cross_entropy(NN.y_hat,y_learning[i])\n",
    "    \n",
    "    loss = loss/len(L)\n",
    "    predicted_labels_validation = NN.predict(x_val)\n",
    "    print('Epoch Summary for epoch:',j)\n",
    "    print('Loss ->',loss[0])\n",
    "    print('accuracy ->',accuracy(predicted_labels_validation,y_val))\n",
    "    print('')\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "NN.test_and_summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
